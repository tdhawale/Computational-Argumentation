{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import timeit\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "# Loading the Spacy Language Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# PLEASE SET TO CORRECT PATH BEFORE RUNNING #\n",
    "#############################################\n",
    "CURRENT_WORKING_DIR = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(\"__file__\")))\n",
    "UNIFIED_DATA_FILE_PATH = f'{CURRENT_WORKING_DIR}/../data/unified_data.json'\n",
    "TRAIN_TEST_SPLIT_FILE_PATH = f'{CURRENT_WORKING_DIR}/../data/train-test-split.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split_dict_and_num_essays():\n",
    "    \"\"\"\n",
    "    Reads the train-test-split.csv file and returns a dict {'essayid' : 'split'} and number of essays set as TRAIN\n",
    "    :return: num_essays : number od essays SET to TRAIN in the train-test-split.csv\n",
    "             train_test_split_dict: a dict of the form {'essayid' : 'split'}\n",
    "    \"\"\"\n",
    "    with open(TRAIN_TEST_SPLIT_FILE_PATH, 'r') as train_file:\n",
    "        train_test_split_dict = {}\n",
    "        num_essays = 0\n",
    "        file_content = train_file.read().split('\\n')[1:-1]\n",
    "        for line in file_content:\n",
    "            essay_id = line.split('\";')[0].split('\"essay')[1]\n",
    "            split = line.split(';\"')[1].split('\"')[0]\n",
    "            train_test_split_dict[essay_id] = split\n",
    "            if split == 'TRAIN':\n",
    "                num_essays += 1\n",
    "        return num_essays, train_test_split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count_dict(all_essays_tokens: dict):\n",
    "    \"\"\"\n",
    "        splits essay tokens\n",
    "        :param all_essays_tokens, a dict of tokens of all essays\n",
    "        :return: dict with essay tokens\n",
    "        \"\"\"\n",
    "    all_essay_tokens = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for essay_id, tokens in all_essays_tokens.items():\n",
    "        words = [token.text.lower() for token in tokens\n",
    "                 if token.is_stop is not True and token.is_punct is not True]\n",
    "        for word in words:\n",
    "            all_essay_tokens[essay_id][word] += 1\n",
    "    return all_essay_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_score(all_argument_units_tokens: dict):\n",
    "    \"\"\"\n",
    "    Computes the TF score for each word in argument units\n",
    "    tf(t) = count of t in argument_unit_text / number of words in argument_unit_text\n",
    "\n",
    "    :param all_argument_units_tokens: tokens list of all 3 argument units in the train-split essays placed in a dict\n",
    "    :return dict of all words with TF scores:\n",
    "    \"\"\"\n",
    "    words_freq = {}\n",
    "    for k, tokens in all_argument_units_tokens.items():\n",
    "        # nlp function returns some weird words like 'educatio' etc which do not exist anywhere in the text\n",
    "        # and their IDF score cannot be computed\n",
    "        words = [token.text.lower() for token in tokens\n",
    "                 if token.is_stop is not True and token.is_punct is not True]\n",
    "        word_count = Counter(words)\n",
    "        tf_scores = {}\n",
    "        for w in word_count:\n",
    "            tf_scores[w] = word_count[w] / len(tokens)\n",
    "        words_freq[k] = tf_scores\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(all_essays_tokens: dict, tf_score_all_arguments: dict):\n",
    "    \"\"\"\n",
    "    Computes the IDF score for each word in  claims, major_claims, premises of all essays\n",
    "    idf(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "    :param all_essays_tokens: dict of tokens of all train-split essays\n",
    "    :param tf_score_all_arguments: a dict of tf scores for each word in claims, major_claims, premises\n",
    "    :return dict of all words with IDF scores: \n",
    "    \"\"\"\n",
    "    all_idf_score = {}\n",
    "    all_essay_tokens = get_token_count_dict(all_essays_tokens)\n",
    "    # For each word appearing in the text of all claims, major claims , and premises - we check in how many essay texts\n",
    "    # this word occurs to calculate the IDF-score of that word based on the above formula\n",
    "    for argument_unit, words in tf_score_all_arguments.items():\n",
    "        for word, _ in words.items():\n",
    "            if word not in all_idf_score:\n",
    "                count = sum([word in all_essay_tokens[essay_id] for essay_id in all_essay_tokens.keys()])\n",
    "                # In order to skip the weird words returned by nlp() which do not appear anywhere in the text(count = 0)\n",
    "                if count > 0:\n",
    "                    all_idf_score[word] = np.log(len(all_essays_tokens) / count)\n",
    "    return all_idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(tf_score_all_arguments, idf_scores):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF score for each word in argument units: claims|major_claims|premises\n",
    "    tf-idf(t) = tf(t) * idf(t)\n",
    "\n",
    "    :param tf_score_all_arguments: tf scores of all words in the argument units: claims, major-claims, premises\n",
    "    :param idf_scores: IDF value of the each word in the argument units: claims, major-claims, premises\n",
    "    :return dict with all words and their TF-IDF scores for current argument unit: \n",
    "    \"\"\"\n",
    "    \n",
    "    tf_idf_scores = {}\n",
    "    for argument_unit, words in tf_score_all_arguments.items():\n",
    "        words_dict = {}\n",
    "        for word, term_freq_score in words.items():\n",
    "            # In order to skip the weird words returned by nlp() which do not appear anywhere in the text(count = 0)\n",
    "            if idf_scores.get(word):\n",
    "                words_dict[word] = term_freq_score * idf_scores[word]\n",
    "        tf_idf_scores[argument_unit] = words_dict\n",
    "    return tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Statistic Variables\n",
    "num_of_paragraphs = 0\n",
    "num_of_sentences = 0\n",
    "num_of_tokens = 0\n",
    "num_of_major_claims = 0\n",
    "num_of_claims = 0\n",
    "num_of_premises = 0\n",
    "num_of_essays_with_conf_bias = 0\n",
    "num_of_essays_without_conf_bias = 0\n",
    "num_of_suff_paras = 0\n",
    "num_of_insuff_paras = 0\n",
    "all_essays_tokens = {}\n",
    "all_argument_units_tokens = {}\n",
    "major_claims_text = []\n",
    "claims_text = []\n",
    "premises_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Essays = Number of Essays in the train-test-split.csv file that have been SET 'TRAIN'\n",
    "# Getting the dict of train-test-split of the form {'essayid' : 'split'}\n",
    "num_of_essays, train_test_split_dict = get_train_test_split_dict_and_num_essays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the unified_data file and calculating some statistics + add texts to appropriate dicts\n",
    "with open(UNIFIED_DATA_FILE_PATH, 'r') as f:\n",
    "    unified_file = json.load(f)\n",
    "    for essay in unified_file:\n",
    "        # We only need to compute for essays SET to 'TRAIN'\n",
    "        if train_test_split_dict[essay['id']] == 'TRAIN':\n",
    "            # Tokenizing the text for the essay using the spaCy library\n",
    "            text = nlp(essay['text'])\n",
    "            all_essays_tokens[essay['id']] = text\n",
    "            num_of_paragraphs += len(essay['paragraphs'])\n",
    "            # Using the spaCy library for calculating Sentences in the text\n",
    "            num_of_sentences += len(list(text.sents))\n",
    "            num_of_tokens += len(text)\n",
    "            num_of_major_claims += len(essay['major_claim'])\n",
    "            num_of_claims += len(essay['claims'])\n",
    "            num_of_premises += len(essay['premises'])\n",
    "            if essay['confirmation_bias']:\n",
    "                num_of_essays_with_conf_bias += 1\n",
    "            else:\n",
    "                num_of_essays_without_conf_bias += 1\n",
    "            for para in essay['paragraphs']:\n",
    "                if para['sufficient']:\n",
    "                    num_of_suff_paras += 1\n",
    "                else:\n",
    "                    num_of_insuff_paras += 1\n",
    "\n",
    "            # Tokenizing using the nlp() of the spaCy library\n",
    "            # Appending the text of the argument unit to a list\n",
    "            for major_claim in essay['major_claim']:\n",
    "                major_claims_text.append(major_claim['text'])\n",
    "            for claim in essay['claims']:\n",
    "                claims_text.append(claim['text'])\n",
    "            for premise in essay['premises']:\n",
    "                premises_text.append(premise['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the argument_units tokens using spaCy library's nlp()\n",
    "major_claims_tokens = nlp(' '.join(major_claims_text))\n",
    "claims_tokens = nlp(' '.join(claims_text))\n",
    "premises_tokens = nlp(' '.join(premises_text))\n",
    "all_argument_units_tokens['major_claim'] = major_claims_tokens\n",
    "all_argument_units_tokens['claims'] = claims_tokens\n",
    "all_argument_units_tokens['premises'] = premises_tokens\n",
    "\n",
    "# Calculating the avg. number of tokens in major_claims, claims, and premises\n",
    "avg_num_of_tokens_in_major_claims = len(major_claims_tokens) / num_of_major_claims\n",
    "avg_num_of_tokens_in_claims = len(claims_tokens) / num_of_claims\n",
    "avg_num_of_tokens_in_premises = len(premises_tokens) / num_of_premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating tf_score for all 3 argument units: major claims | claims | premises\n",
    "tf_score_all_arguments = tf_score(all_argument_units_tokens)\n",
    "\n",
    "# calculate IDF score for each word in the whole text of all claims, major-claims and premises\n",
    "idf_scores = idf(all_essays_tokens, tf_score_all_arguments)\n",
    "\n",
    "# calculate TF-IDF score for all words in major claims | claims | premises\n",
    "all_tf_idf_scores = tf_idf(tf_score_all_arguments, idf_scores)\n",
    "\n",
    "# get the top 10 scores for each \n",
    "major_claims_ten_specific_words = Counter(all_tf_idf_scores['major_claim']).most_common(10)\n",
    "claims_ten_specific_words = Counter(all_tf_idf_scores['claims']).most_common(10)\n",
    "premises_ten_specific_words = Counter(all_tf_idf_scores['premises']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Preliminary Statistics are:\n",
      "Number of essays: 322\n",
      "Number of paragraphs: 820\n",
      "Number of sentences: 5462\n",
      "Number of tokens: 116588\n",
      "Number of major claims: 598\n",
      "Number of claims: 1202\n",
      "Number of premises: 3023\n",
      "Number of essays with confirmation bias: 122\n",
      "Number of essays without confirmation bias: 200\n",
      "Number of sufficient paragraphs: 538\n",
      "Number of insufficient paragraphs: 282\n",
      "Average number of tokens in major claims: 14.695652173913043\n",
      "Average number of tokens in claims: 15.090682196339435\n",
      "Average number of tokens in premises: 17.60205094277208\n",
      "\n",
      "10 most specific words in major claims:\n",
      "1) 'education' -- TF-IDF score: 0.007814352942339236\n",
      "2) 'students' -- TF-IDF score: 0.006820153960791321\n",
      "3) 'children' -- TF-IDF score: 0.005556873203441235\n",
      "4) 'prefer' -- TF-IDF score: 0.005349708521619993\n",
      "5) 'health' -- TF-IDF score: 0.005032478596686616\n",
      "6) 'learning' -- TF-IDF score: 0.004771970871660055\n",
      "7) 'government' -- TF-IDF score: 0.0047025113986408585\n",
      "8) 'parents' -- TF-IDF score: 0.004604851886289081\n",
      "9) 'television' -- TF-IDF score: 0.004349158797165885\n",
      "10) 'school' -- TF-IDF score: 0.004090720327341884\n",
      "\n",
      "10 most specific words in claims:\n",
      "1) 'students' -- TF-IDF score: 0.00822387300149172\n",
      "2) 'children' -- TF-IDF score: 0.006141579478203792\n",
      "3) 'television' -- TF-IDF score: 0.0042141691944973594\n",
      "4) 'health' -- TF-IDF score: 0.0037587992047159744\n",
      "5) 'technology' -- TF-IDF score: 0.0037366075807716047\n",
      "6) 'new' -- TF-IDF score: 0.003729112785029548\n",
      "7) 'parents' -- TF-IDF score: 0.0037182717144190643\n",
      "8) 'life' -- TF-IDF score: 0.0036510343761798104\n",
      "9) 'living' -- TF-IDF score: 0.0035938309115204356\n",
      "10) 'tv' -- TF-IDF score: 0.0035885434162133117\n",
      "\n",
      "10 most specific words in premises:\n",
      "1) 'students' -- TF-IDF score: 0.0059072084384199815\n",
      "2) 'children' -- TF-IDF score: 0.003556238026599502\n",
      "3) 'work' -- TF-IDF score: 0.0030179058241297423\n",
      "4) 'job' -- TF-IDF score: 0.0028513233012199997\n",
      "5) 'animals' -- TF-IDF score: 0.00282416246945972\n",
      "6) 'time' -- TF-IDF score: 0.0027392046147288905\n",
      "7) 'country' -- TF-IDF score: 0.002611200485397113\n",
      "8) 'friends' -- TF-IDF score: 0.0025194811980503357\n",
      "9) 'money' -- TF-IDF score: 0.002429609669876764\n",
      "10) 'knowledge' -- TF-IDF score: 0.0024150850730631428\n"
     ]
    }
   ],
   "source": [
    "print(\"The Preliminary Statistics are:\")\n",
    "print(\"Number of essays: {}\".format(num_of_essays))\n",
    "print(\"Number of paragraphs: {}\".format(num_of_paragraphs))\n",
    "print(\"Number of sentences: {}\".format(num_of_sentences))\n",
    "print(\"Number of tokens: {}\".format(num_of_tokens))\n",
    "print(\"Number of major claims: {}\".format(num_of_major_claims))\n",
    "print(\"Number of claims: {}\".format(num_of_claims))\n",
    "print(\"Number of premises: {}\".format(num_of_premises))\n",
    "print(\"Number of essays with confirmation bias: {}\".format(num_of_essays_with_conf_bias))\n",
    "print(\"Number of essays without confirmation bias: {}\".format(num_of_essays_without_conf_bias))\n",
    "print(\"Number of sufficient paragraphs: {}\".format(num_of_suff_paras))\n",
    "print(\"Number of insufficient paragraphs: {}\".format(num_of_insuff_paras))\n",
    "print(\"Average number of tokens in major claims: {}\".format(avg_num_of_tokens_in_major_claims))\n",
    "print(\"Average number of tokens in claims: {}\".format(avg_num_of_tokens_in_claims))\n",
    "print(\"Average number of tokens in premises: {}\".format(avg_num_of_tokens_in_premises))\n",
    "\n",
    "print(\"\\n10 most specific words in major claims:\")\n",
    "for i, word in enumerate(major_claims_ten_specific_words):\n",
    "    print(\"{}) '{}' -- TF-IDF score: {}\".format(i+1, word[0], word[1]))\n",
    "print(\"\\n10 most specific words in claims:\")\n",
    "for i, word in enumerate(claims_ten_specific_words):\n",
    "    print(\"{}) '{}' -- TF-IDF score: {}\".format(i+1, word[0], word[1]))\n",
    "print(\"\\n10 most specific words in premises:\")\n",
    "for i, word in enumerate(premises_ten_specific_words):\n",
    "    print(\"{}) '{}' -- TF-IDF score: {}\".format(i+1, word[0], word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
